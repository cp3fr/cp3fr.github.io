[{"authors":null,"categories":null,"content":"I am a postdoctoral researcher at the University of Zurich Robotics \u0026amp; Perception Group led by Prof. Davide Scaramuzza.\nMy research interest lie in the intersection of neuroscience, machine learning and robotics. I currently focus on visual-motor coordination and decision-making processes in first-person view drone racing.\n","date":1617235200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617235200,"objectID":"b3a620fbb1b943d23e2b1a75fb2e96b1","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a postdoctoral researcher at the University of Zurich Robotics \u0026amp; Perception Group led by Prof. Davide Scaramuzza.\nMy research interest lie in the intersection of neuroscience, machine learning and robotics.","tags":null,"title":"Christian Pfeiffer","type":"authors"},{"authors":null,"categories":null,"content":"I am a postdoctoral researcher at the University of Zurich Robotics \u0026amp; Perception Group led by Prof. Davide Scaramuzza.\nMy research interest lie in the intersection of neuroscience, machine learning and robotics. I currently focus on visual-motor coordination and decision-making processes in first-person view drone racing.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a postdoctoral researcher at the University of Zurich Robotics \u0026amp; Perception Group led by Prof. Davide Scaramuzza.\nMy research interest lie in the intersection of neuroscience, machine learning and robotics.","tags":null,"title":"Christian Pfeiffer","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://cp3fr.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Christian Pfeiffer","Davide Scaramuzza"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"ebc16f6a7adc027870f2ac274ecf6d90","permalink":"https://cp3fr.github.io/publication/pfeiffer-2021-a/","publishdate":"2021-04-07T06:17:34.717509Z","relpermalink":"/publication/pfeiffer-2021-a/","section":"publication","summary":"Humans race drones faster and more agile than algorithms, despite being limited to a fixed camera angle, body rate control, and response latencies in the order of hundreds of milliseconds. A better understanding of the ability of human pilots of selecting appropriate motor commands from highly dynamic visual information may provide key insights for solving current challenges in vision-based autonomous navigation. The aim of this study was to investigate the relationship between flight performance, control behavior, and eye movements of human pilots in a drone racing task. We collected a multimodal dataset from 21 experienced drone pilots using a highly realistic drone racing simulator, also used to recruit professional pilots. Our results showed task-specific improvements in drone racing performance over time. Gaze fixations not only tracked future waypoints but also anticipated the future flight path. Cross-correlation analysis showed a strong spatio-temporal relationship between eye movements, camera orienting behavior, and thrust vector control. These results highlight the importance of coordinated eye movements in human-piloted drone racing.","tags":["Aerial Systems: Perception and Autonomy","Automobiles","Drones","Eye-Tracking","Human Factors and Human-in-the-Loop","Logic gates","Perception-Action Coupling","Roads","Tracking","Vehicles","Vision-Based Navigation","Visualization"],"title":"Human-Piloted Drone Racing: Visual Processing and Control","type":"publication"},{"authors":["Christian Pfeiffer","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://cp3fr.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Christian Pfeiffer","Nora Hollenstein","Ce Zhang","Nicolas Langer"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"512ce24826b9ca5d073757240eebaa14","permalink":"https://cp3fr.github.io/publication/pfeiffer-2020/","publishdate":"2021-04-07T06:17:34.718144Z","relpermalink":"/publication/pfeiffer-2020/","section":"publication","summary":"When we read, our eyes move through the text in a series of fixations and high-velocity saccades to extract visual information. This process allows the brain to obtain meaning, e.g., about sentiment, or the emotional valence, expressed in the written text. How exactly the brain extracts the sentiment of single words during naturalistic reading is largely unknown. This is due to the challenges of naturalistic imaging, which has previously led researchers to employ highly controlled, timed word-by-word presentations of custom reading materials that lack ecological validity. Here, we aimed to assess the electrical neural correlates of word sentiment processing during naturalistic reading of English sentences. We used a publicly available dataset of simultaneous electroencephalography (EEG), eye-tracking recordings, and word-level semantic annotations from 7129 words in 400 sentences (Zurich Cognitive Language Processing Corpus; Hollenstein et al., 2018). We computed fixation-related potentials (FRPs), which are evoked electrical responses time-locked to the onset of fixations. A general linear mixed model analysis of FRPs cleaned from visual- and motor-evoked activity showed a topographical difference between the positive and negative sentiment condition in the 224‚Äì304 ‚Äãms interval after fixation onset in left-central and right-posterior electrode clusters. An additional analysis that included word-, phrase-, and sentence-level sentiment predictors showed the same FRP differences for the word-level sentiment, but no additional FRP differences for phrase- and sentence-level sentiment. Furthermore, decoding analysis that classified word sentiment (positive or negative) from sentiment-matched 40-trial average FRPs showed a 0.60 average accuracy (95% confidence interval: [0.58, 0.61]). Control analyses ruled out that these results were based on differences in eye movements or linguistic features other than word sentiment. Our results extend previous research by showing that the emotional valence of lexico-semantic stimuli evoke a fast electrical neural response upon word fixation during naturalistic reading. These results provide an important step to identify the neural processes of lexico-semantic processing in ecologically valid conditions and can serve to improve computer algorithms for natural language processing.","tags":null,"title":"Neural dynamics of sentiment processing during naturalistic sentence reading","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cp3fr.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Christian Pfeiffer","Nathalie Ata Nguepno Nguissi","Magali Chytiris","Phanie Bidlingmeyer","Matthias Haenggi","Rebekka Kurmann","Fr√©d√©ric Zubler","Mauro Oddo","Andrea O. Rossetti","Marzia De Lucia"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"622ca9426262dbdf74447a4888d84309","permalink":"https://cp3fr.github.io/publication/pfeiffer-2017/","publishdate":"2021-04-07T06:17:34.718625Z","relpermalink":"/publication/pfeiffer-2017/","section":"publication","summary":"Background Outcome prognostication in postanoxic comatose patients is more accurate in predicting poor than good recovery. Using electroencephalography recordings in patients treated with targeted temperature management at 33 ¬∞C (TTM 33), we have previously shown that improvement in auditory discrimination over the first days of coma predicted awakening. Given the increased application of a 36 ¬∞C temperature target (TTM 36), here we aimed at validating the predictive value of auditory discrimination in the TTM 36 setting. Methods In this prospective multicenter study, we analyzed the EEG responses to auditory stimuli from 60 consecutive patients from the first and second coma day. A semiautomatic decoding analysis was applied to single patient data to quantify discrimination performance between frequently repeated and deviant sounds. The decoding change from the first to second day was used for predicting patient outcome. Results We observed an increase in auditory discrimination in 25 out of 60 patients. Among them, 17 awoke from coma (68% positive predictive value; 95% confidence interval: 0.46‚Äì0.85). By excluding patients with electroencephalographic epileptiform features, 15 of 18 exhibited improvement in auditory discrimination (83% positive predictive value; 95% confidence interval: 0.59‚Äì0.96). Specificity of good outcome prediction increased after adding auditory discrimination to EEG reactivity. Conclusion These results suggest that tracking of auditory discrimination over time is informative of good recovery independent of the temperature target. This quantitative test provides complementary information to existing clinical tools by identifying patients with high chances of recovery and encouraging the maintenance of life support.","tags":["Cardiac arrest","Coma","EEG","Mismatch negativity","Multivariate decoding","Targeted temperature management"],"title":"Auditory discrimination improvement predicts awakening of postanoxic comatose patients treated with targeted temperature management at 36 ¬∞C","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://cp3fr.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Christian Pfeiffer","Michiel van Elk","Fosco Bernasconi","Olaf Blanke"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"cdea5f3d93d49a53400a5a8075d3efba","permalink":"https://cp3fr.github.io/publication/pfeiffer-2016/","publishdate":"2021-04-07T06:17:34.719407Z","relpermalink":"/publication/pfeiffer-2016/","section":"publication","summary":"In non-human primates several brain areas contain neurons that respond to both vestibular and somatosensory stimulation. In humans, vestibular stimulation activates several somatosensory brain regions and improves tactile perception. However, less is known about the spatio-temporal dynamics of such vestibular-somatosensory interactions in the human brain. To address this issue, we recorded high-density electroencephalography during left median nerve electrical stimulation to obtain Somatosensory Evoked Potentials (SEPs). We analyzed SEPs during vestibular activation following sudden decelerations from constant-velocity (90¬∞/s and 60¬∞/s) earth-vertical axis yaw rotations and SEPs during a non-vestibular control period. SEP analysis revealed two distinct temporal effects of vestibular activation: An early effect (28-32 ms post-stimulus) characterized by vestibular suppression of SEP response strength that depended on rotation velocity and a later effect (97-112 ms post-stimulus) characterized by vestibular modulation of SEP topographical pattern that was rotation velocity-independent. Source estimation localized these vestibular effects, during both time periods, to activation differences in a distributed cortical network including the right postcentral gyrus, right insula, left precuneus, and bilateral secondary somatosensory cortex. These results suggest that vestibular-somatosensory interactions in humans depend on processing in specific time periods in somatosensory and vestibular cortical regions.","tags":["EEG","Electrical neuroimaging","Multisensory processing","Somatosensory cortex","Somatosensory evoked potentials","Vestibular system"],"title":"Distinct vestibular effects on early and late somatosensory cortical processing in humans","type":"publication"},{"authors":["Christian Pfeiffer","Petr Grivaz","Bruno Herbelin","Andrea Serino","Olaf Blanke"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"94314cb1a2f5119cc6f343b89d8c137e","permalink":"https://cp3fr.github.io/publication/pfeiffer-2016-a/","publishdate":"2021-04-07T06:17:34.719018Z","relpermalink":"/publication/pfeiffer-2016-a/","section":"publication","summary":"Why should a scientist whose aim is to unravel the neural mechanisms of perception consider brain-body interactions seriously? Brain-body interactions have traditionally been associated with emotion, effort, or stress, but not with the ‚Äúcold‚Äù processes of perception and attention. Here, we review recent experimental evidence suggesting a different picture: the neural monitoring of bodily state, and in particular the neural monitoring of the heart, affects visual perception. The impact of spontaneous fluctuations of neural responses to heartbeats on visual detection is as large as the impact of explicit manipulations of spatial attention in perceptual tasks. However, we propose that the neural monitoring of visceral inputs plays a specific role in conscious perception, distinct from the role of attention. The neural monitoring of organs such as the heart or the gut would generate a subject-centered reference frame, from which the first-person perspective inherent to conscious perception can develop. In this view, conscious perception results from the integration of visual content with first-person perspective.","tags":["first-person perspective","full-body illusion","gravity","multisensory integration","self-consciousness","virtual reality"],"title":"Visual gravity contributes to subjective first-person perspective","type":"publication"},{"authors":["Christian Pfeiffer","Andrea Serino","Olaf Blanke"],"categories":null,"content":"","date":1396310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396310400,"objectID":"2162a034017cbc4a95d19cd88633a8e9","permalink":"https://cp3fr.github.io/publication/pfeiffer-2014-a/","publishdate":"2021-04-07T06:17:34.719856Z","relpermalink":"/publication/pfeiffer-2014-a/","section":"publication","summary":"Self-consciousness is the remarkable human experience of being a subject: the \"I\". Self-consciousness is typically bound to a body, and particularly to the spatial dimensions of the body, as well as to its location and displacement in the gravitational field. Because the vestibular system encodes head position and movement in three-dimensional space, vestibular cortical processing likely contributes to spatial aspects of bodily self-consciousness. We review here recent data showing vestibular effects on first-person perspective (the feeling from where \"I\" experience the world) and self-location (the feeling where \"I\" am located in space). We compare these findings to data showing vestibular effects on mental spatial transformation, self-motion perception, and body representation showing vestibular contributions to various spatial representations of the body with respect to the external world. Finally, we discuss the role for four posterior brain regions that process vestibular and other multisensory signals to encode spatial aspects of bodily self-consciousness: temporoparietal junction, parietoinsular vestibular cortex, ventral intraparietal region, and medial superior temporal region. We propose that vestibular processing in these cortical regions is critical in linking multisensory signals from the body (personal and peripersonal space) with external (extrapersonal) space. Therefore, the vestibular system plays a critical role for neural representations of spatial aspects of bodily self-consciousness. ¬© 2014 Pfeiffer, Serino and Blanke.","tags":["Bodily self-consciousness","Body representation","First-person perspective","Mental spatial transformation","Multisensory integration","Self-location","Self-motion","Vestibular cortex"],"title":"The vestibular system: a spatial reference for bodily self-consciousness","type":"publication"},{"authors":["Christian Pfeiffer","Christophe Lopez","Valentin Schmutz","Julio Angel Duenas","Roberto Martuzzi","Olaf Blanke"],"categories":null,"content":"","date":1364774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1364774400,"objectID":"d0281fdaa557b67117ff275952c2d03d","permalink":"https://cp3fr.github.io/publication/pfeiffer-2013/","publishdate":"2021-04-07T06:17:34.720143Z","relpermalink":"/publication/pfeiffer-2013/","section":"publication","summary":"In three experiments we investigated the effects of visuo-tactile and visuo-vestibular conflict about the direction of gravity on three aspects of bodily self-consciousness: self-identification, self-location, and the experienced direction of the first-person perspective. Robotic visuo-tactile stimulation was administered to 78 participants in three experiments. Additionally, we presented participants with a virtual body as seen from an elevated and downward-directed perspective while they were lying supine and were therefore receiving vestibular and postural cues about an upward-directed perspective. Under these conditions, we studied the effects of different degrees of visuo-vestibular conflict, repeated measurements during illusion induction, and the relationship to a classical measure of visuo-vestibular integration. Extending earlier findings on experimentally induced changes in bodily self-consciousness, we show that self-identification does not depend on the experienced direction of the first-person perspective, whereas self-location does. Changes in bodily self-consciousness depend on visual gravitational signals. Individual differences in the experienced direction of first-person perspective correlated with individual differences in visuo-vestibular integration. Our data reveal important contributions of visuo-vestibular gravitational cues to bodily self-consciousness. In particular we show that the experienced direction of the first-person perspective depends on the integration of visual, vestibular, and tactile signals, as well as on individual differences in idiosyncratic visuo-vestibular strategies. ¬© 2013 Pfeiffer et al.","tags":null,"title":"Multisensory Origin of the Subjective First-Person Perspective: Visual, Tactile, and Vestibular Mechanisms","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://cp3fr.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]