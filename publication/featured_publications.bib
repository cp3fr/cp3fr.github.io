@article{Pfeiffer2021,
abstract = {Humans race drones faster and more agile than algorithms, despite being limited to a fixed camera angle, body rate control, and response latencies in the order of hundreds of milliseconds. A better understanding of the ability of human pilots of selecting appropriate motor commands from highly dynamic visual information may provide key insights for solving current challenges in vision-based autonomous navigation. The aim of this study was to investigate the relationship between flight performance, control behavior, and eye movements of human pilots in a drone racing task. We collected a multimodal dataset from 21 experienced drone pilots using a highly realistic drone racing simulator, also used to recruit professional pilots. Our results showed task-specific improvements in drone racing performance over time. Gaze fixations not only tracked future waypoints but also anticipated the future flight path. Cross-correlation analysis showed a strong spatio-temporal relationship between eye movements, camera orienting behavior, and thrust vector control. These results highlight the importance of coordinated eye movements in human-piloted drone racing.},
archivePrefix = {arXiv},
arxivId = {2103.04672},
author = {Pfeiffer, Christian and Scaramuzza, Davide},
doi = {10.1109/LRA.2021.3064282},
eprint = {2103.04672},
file = {:home/cp3fr/Mendeley/library/Pfeiffer, Scaramuzza{\_}2021{\_}Human-Piloted Drone Racing Visual Processing and Control(2).pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial Systems: Perception and Autonomy,Automobiles,Drones,Eye-Tracking,Human Factors and Human-in-the-Loop,Logic gates,Perception-Action Coupling,Roads,Tracking,Vehicles,Vision-Based Navigation,Visualization},
month = {apr},
number = {2},
pages = {3467--3474},
title = {{Human-Piloted Drone Racing: Visual Processing and Control}},
url = {https://ieeexplore.ieee.org/document/9372809/},
volume = {6},
year = {2021}
}
@article{Pfeiffer2020,
abstract = {When we read, our eyes move through the text in a series of fixations and high-velocity saccades to extract visual information. This process allows the brain to obtain meaning, e.g., about sentiment, or the emotional valence, expressed in the written text. How exactly the brain extracts the sentiment of single words during naturalistic reading is largely unknown. This is due to the challenges of naturalistic imaging, which has previously led researchers to employ highly controlled, timed word-by-word presentations of custom reading materials that lack ecological validity. Here, we aimed to assess the electrical neural correlates of word sentiment processing during naturalistic reading of English sentences. We used a publicly available dataset of simultaneous electroencephalography (EEG), eye-tracking recordings, and word-level semantic annotations from 7129 words in 400 sentences (Zurich Cognitive Language Processing Corpus; Hollenstein et al., 2018). We computed fixation-related potentials (FRPs), which are evoked electrical responses time-locked to the onset of fixations. A general linear mixed model analysis of FRPs cleaned from visual- and motor-evoked activity showed a topographical difference between the positive and negative sentiment condition in the 224–304 ​ms interval after fixation onset in left-central and right-posterior electrode clusters. An additional analysis that included word-, phrase-, and sentence-level sentiment predictors showed the same FRP differences for the word-level sentiment, but no additional FRP differences for phrase- and sentence-level sentiment. Furthermore, decoding analysis that classified word sentiment (positive or negative) from sentiment-matched 40-trial average FRPs showed a 0.60 average accuracy (95{\%} confidence interval: [0.58, 0.61]). Control analyses ruled out that these results were based on differences in eye movements or linguistic features other than word sentiment. Our results extend previous research by showing that the emotional valence of lexico-semantic stimuli evoke a fast electrical neural response upon word fixation during naturalistic reading. These results provide an important step to identify the neural processes of lexico-semantic processing in ecologically valid conditions and can serve to improve computer algorithms for natural language processing.},
author = {Pfeiffer, Christian and Hollenstein, Nora and Zhang, Ce and Langer, Nicolas},
doi = {10.1016/j.neuroimage.2020.116934},
file = {:home/cp3fr/Mendeley/library/Pfeiffer et al.{\_}2020{\_}Neural dynamics of sentiment processing during naturalistic sentence reading.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
month = {sep},
number = {April},
pages = {116934},
pmid = {32416227},
publisher = {The Authors},
title = {{Neural dynamics of sentiment processing during naturalistic sentence reading}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811920304201},
volume = {218},
year = {2020}
}
@article{Pfeiffer2018,
abstract = {Human–environment interactions are mediated through the body and occur within the peripersonal space (PPS), the space immediately adjacent to and surrounding the body. The PPS is taken to be a critical interface between the body and the environment, and indeed, body-part specific PPS remapping has been shown to depend on body-part utilization, such as upper limb movements in otherwise static observers. How vestibular signals induced by whole-body movement contribute to PPS representation is less well understood. In a series of experiments, we mapped the spatial extension of the PPS around the head while participants were submitted to passive whole-body rotations inducing vestibular stimulation. Forty-six participants, in three experiments, executed a tactile detection reaction time task while task-irrelevant auditory stimuli approached them. The maximal distance at which the auditory stimulus facilitated tactile reaction time was taken as a proxy for the boundary of peri-head space. The present results indicate two distinct vestibular effects. First, vestibular stimulation speeded tactile detection indicating a vestibular facilitation of somatosensory processing. Second, vestibular stimulation modulated audio-tactile interaction of peri-head space in a rotation direction-specific manner. Congruent but not incongruent audio-vestibular motion stimuli expanded the PPS boundary further away from the body as compared to no rotation. These results show that vestibular inputs dynamically update the multisensory delineation of PPS and far space, which may serve to maintain accurate tracking of objects close to the body and to update spatial self-representations.},
author = {Pfeiffer, Christian and Noel, Jean‐Paul and Serino, Andrea and Blanke, Olaf},
doi = {10.1111/ejn.13872},
file = {:home/cp3fr/Mendeley/library/Pfeiffer et al.{\_}2018{\_}Vestibular modulation of peripersonal space boundaries.pdf:pdf},
issn = {0953-816X},
journal = {European Journal of Neuroscience},
keywords = {humans,multisensory processing,peripersonal space,self-motion,vestibular system},
month = {apr},
number = {7},
pages = {800--811},
pmid = {29461657},
title = {{Vestibular modulation of peripersonal space boundaries}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.13872},
volume = {47},
year = {2018}
}
@article{Pfeiffer2017,
abstract = {Successful prediction of future events depends on the brain's capacity to extract temporal regularities from sensory inputs. Neuroimaging studies mainly investigated regularity processing for exteroceptive sensory inputs (i.e. from outside the body). Here we investigated whether interoceptive signals (i.e. from inside the body) can mediate auditory regularity processing. Human participants passively listened to sound sequences presented in synchrony or asynchrony to their heartbeat while concomitant electroencephalography was recorded. We hypothesized that the cardio-audio synchronicity would induce a brain expectation of future sounds. Electrical neuroimaging analysis revealed a surprise response at 158-270 ms upon omission of the expected sounds in the synchronous condition only. Control analyses ruled out that this effect was trivially based on expectation from the auditory temporal structure or on differences in heartbeat physiological signals. Implicit neural monitoring of temporal regularities across interoceptive and exteroceptive signals drives prediction of future events in auditory sequences.},
author = {Pfeiffer, Christian and {De Lucia}, Marzia},
doi = {10.1038/s41598-017-13861-8},
file = {:home/cp3fr/Mendeley/library/Pfeiffer, De Lucia{\_}2017{\_}Cardio-audio synchronization drives neural surprise response.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {14842},
pmid = {29093486},
publisher = {Springer US},
title = {{Cardio-audio synchronization drives neural surprise response}},
url = {http://dx.doi.org/10.1038/s41598-017-13861-8 http://www.nature.com/articles/s41598-017-13861-8},
volume = {7},
year = {2017}
}
@article{Pfeiffer2016,
abstract = {In non-human primates several brain areas contain neurons that respond to both vestibular and somatosensory stimulation. In humans, vestibular stimulation activates several somatosensory brain regions and improves tactile perception. However, less is known about the spatio-temporal dynamics of such vestibular-somatosensory interactions in the human brain. To address this issue, we recorded high-density electroencephalography during left median nerve electrical stimulation to obtain Somatosensory Evoked Potentials (SEPs). We analyzed SEPs during vestibular activation following sudden decelerations from constant-velocity (90°/s and 60°/s) earth-vertical axis yaw rotations and SEPs during a non-vestibular control period. SEP analysis revealed two distinct temporal effects of vestibular activation: An early effect (28-32 ms post-stimulus) characterized by vestibular suppression of SEP response strength that depended on rotation velocity and a later effect (97-112 ms post-stimulus) characterized by vestibular modulation of SEP topographical pattern that was rotation velocity-independent. Source estimation localized these vestibular effects, during both time periods, to activation differences in a distributed cortical network including the right postcentral gyrus, right insula, left precuneus, and bilateral secondary somatosensory cortex. These results suggest that vestibular-somatosensory interactions in humans depend on processing in specific time periods in somatosensory and vestibular cortical regions.},
author = {Pfeiffer, Christian and van Elk, Michiel and Bernasconi, Fosco and Blanke, Olaf},
doi = {10.1016/j.neuroimage.2015.10.004},
file = {:home/cp3fr/Mendeley/library/Pfeiffer et al.{\_}2016{\_}Distinct vestibular effects on early and late somatosensory cortical processing in humans.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {EEG,Electrical neuroimaging,Multisensory processing,Somatosensory cortex,Somatosensory evoked potentials,Vestibular system},
month = {jan},
pages = {208--219},
pmid = {26466979},
publisher = {Elsevier Inc.},
title = {{Distinct vestibular effects on early and late somatosensory cortical processing in humans}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2015.10.004 https://linkinghub.elsevier.com/retrieve/pii/S1053811915008988},
volume = {125},
year = {2016}
}
@article{Pfeiffer2014,
abstract = {Self-consciousness is the remarkable human experience of being a subject: the "I". Self-consciousness is typically bound to a body, and particularly to the spatial dimensions of the body, as well as to its location and displacement in the gravitational field. Because the vestibular system encodes head position and movement in three-dimensional space, vestibular cortical processing likely contributes to spatial aspects of bodily self-consciousness. We review here recent data showing vestibular effects on first-person perspective (the feeling from where "I" experience the world) and self-location (the feeling where "I" am located in space). We compare these findings to data showing vestibular effects on mental spatial transformation, self-motion perception, and body representation showing vestibular contributions to various spatial representations of the body with respect to the external world. Finally, we discuss the role for four posterior brain regions that process vestibular and other multisensory signals to encode spatial aspects of bodily self-consciousness: temporoparietal junction, parietoinsular vestibular cortex, ventral intraparietal region, and medial superior temporal region. We propose that vestibular processing in these cortical regions is critical in linking multisensory signals from the body (personal and peripersonal space) with external (extrapersonal) space. Therefore, the vestibular system plays a critical role for neural representations of spatial aspects of bodily self-consciousness. {\textcopyright} 2014 Pfeiffer, Serino and Blanke.},
author = {Pfeiffer, Christian and Serino, Andrea and Blanke, Olaf},
doi = {10.3389/fnint.2014.00031},
file = {:home/cp3fr/Mendeley/library/Pfeiffer, Serino, Blanke{\_}2014{\_}The vestibular system a spatial reference for bodily self-consciousness.pdf:pdf},
issn = {1662-5145},
journal = {Frontiers in Integrative Neuroscience},
keywords = {Bodily self-consciousness,Body representation,First-person perspective,Mental spatial transformation,Multisensory integration,Self-location,Self-motion,Vestibular cortex},
month = {apr},
number = {APR},
pages = {1--13},
title = {{The vestibular system: a spatial reference for bodily self-consciousness}},
url = {http://journal.frontiersin.org/article/10.3389/fnint.2014.00031/abstract},
volume = {8},
year = {2014}
}
@article{Pfeiffer2013,
abstract = {In three experiments we investigated the effects of visuo-tactile and visuo-vestibular conflict about the direction of gravity on three aspects of bodily self-consciousness: self-identification, self-location, and the experienced direction of the first-person perspective. Robotic visuo-tactile stimulation was administered to 78 participants in three experiments. Additionally, we presented participants with a virtual body as seen from an elevated and downward-directed perspective while they were lying supine and were therefore receiving vestibular and postural cues about an upward-directed perspective. Under these conditions, we studied the effects of different degrees of visuo-vestibular conflict, repeated measurements during illusion induction, and the relationship to a classical measure of visuo-vestibular integration. Extending earlier findings on experimentally induced changes in bodily self-consciousness, we show that self-identification does not depend on the experienced direction of the first-person perspective, whereas self-location does. Changes in bodily self-consciousness depend on visual gravitational signals. Individual differences in the experienced direction of first-person perspective correlated with individual differences in visuo-vestibular integration. Our data reveal important contributions of visuo-vestibular gravitational cues to bodily self-consciousness. In particular we show that the experienced direction of the first-person perspective depends on the integration of visual, vestibular, and tactile signals, as well as on individual differences in idiosyncratic visuo-vestibular strategies. {\textcopyright} 2013 Pfeiffer et al.},
author = {Pfeiffer, Christian and Lopez, Christophe and Schmutz, Valentin and Duenas, Julio Angel and Martuzzi, Roberto and Blanke, Olaf},
doi = {10.1371/journal.pone.0061751},
editor = {Tsakiris, Manos},
file = {:home/cp3fr/Mendeley/library/Pfeiffer et al.{\_}2013{\_}Multisensory Origin of the Subjective First-Person Perspective Visual, Tactile, and Vestibular Mechanisms.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
month = {apr},
number = {4},
pages = {e61751},
pmid = {23630611},
title = {{Multisensory Origin of the Subjective First-Person Perspective: Visual, Tactile, and Vestibular Mechanisms}},
url = {https://dx.plos.org/10.1371/journal.pone.0061751},
volume = {8},
year = {2013}
}
